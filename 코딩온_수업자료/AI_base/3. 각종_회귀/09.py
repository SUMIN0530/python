# 경사하강법
# 가장 낮은 곳을 찾아가는 방법

# 예
# 안개 낀 산에서 내려오기
# 안개 낀 산 정상 -> 마을은 가장 낮은 곳에 존재
# but, 앞이 보이지 않아 어디로 가야할 지 모름
# 때문에 발 밑 지형만 느끼면서 
# "지금 서 있는 곳에서 가장 가파르게 내려가는 방향"을 찾아 내려감
# => 경사 하강법의 원리

# 머신러닝에서 모델을 학습시킨다는 것 = "최적의 파라미터 값을 찾는다"
# 예를 들면 집 가격을 예측하는 모델이 있다면, 에측값과 실제 가격의 차이(오차)를
# 최소화하는 파라미터를 찾아야 함
# 
# 이 오차를 수치화한 것이 손실 함수
# 경사 하강법은 이 손실 함수의 값을 최소화하는 파라미터를 찾아가는 알고리즘

# 작동 원리
# 경사 하강법
# θ(새로운) = θ(현재) - α x ▽L(θ)
# 
# θ(세타) : 우리가 찾고자 하는 파라미터
# α(알파) : 학습률, 한 번에 얼마나 크게 이동할지 결정
# ▽L(θ) : 현재 위치에서 손실 함수의 기울기(gradient) 

# 기울기에 마이너스를 붙이는 이유 : 기울기는 함수가 증가하는 방향으로 가리킴
# 따라서 반대로 가야 감소하는 방향이 됨

# 학습률의 중요성
# 학습률 α는 매우 중요한 하이퍼퍼라미터  => 머신러닝 모델에서 사용자가 직접 조절하는 범위?
# 
# 학습률이 너무 크면 최저점을 지나쳐서 발산할 수 있음
# 산을 내려오는데 너무 큰 보폭으로 뛰어다니면 반대편 산으로 넘어감
# 
# 학습률이 너무 작으면 수럼하는데 너무 오래 걸림
# 아주 작은 걸음으로 조금씩 이동하니 시간이 판참 걸림 

# 한계점
# 지역 최솟값 문제
# 경사 하강법은 "현재 위치에서 가장 가파른 방향"만 보고 이동
# 따라서 전체에서 가장 낮은 곳(전역 최솟값, Global Minimum)이 아니라 
# 근처 움품 파인 곳(지역 최솟값, Local Minimum)에 갇힐 수 있다

# 한계 극복
# SGD(확률적 경사 하강법) : 전체 데이터 대신 일부 샘플만 사용
# 더 빠른 업데이트, 약간의 무작위성으로 지역 최솟값 탈출에 도움을 준다
# 
# Momentum : 이전 이동 방향의 관성을 반영해서 지역 최솟값을 넘어갈 수 있게 해줌
# 
# Adam : 학습률을 파라미터별로 적응적으로 조절하는 방법으로
# 현재 가장 널리 쓰이는 옵티마이저 중 하나
import numpy as np
import matplotlib.pyplot as plt

# 데이터 생성
np.random.seed(42)
x = np.random.randn(100)
y = 2 * x + 3 + np.random.randn(100) * 0.5 # y = 2x + 3 + 노이즈

# 파라미터 초기화
w = 0.0
b = 0.0
lr = 0.1
epochs = 100

# 기록용
loss_history = []
w_history =[]
b_history =[]

# 경사 하강법
for epoch in range(epochs):
    # 예측 
    y_pred = w * x + b

    # 손실 계산
    loss = np.mean((y - y_pred) ** 2)
    loss_history.append(loss)

    # 그래디언트
    dw = np.mean(2 * (y_pred - y) * x)
    db = np.mean(2 * (y_pred - y))

    # 파라미터 업데이트
    w = w - lr * dw
    b = b - lr * db

    # 기록
    w_history.append(w)
    b_history.append(b)

    if epoch % 20 == 0:
        print(f'Epoch {epoch:3d}: Loss={loss:.4f}, w={w:.4f}, b={b:.4f}')
print(f'최종 : w={w:.4f} (목표 : 2), b={b:.4f} (목표 : 3)')