# 인공 신경망의 구조

# 생물학적 뉴런 vs 인공 뉴런\

# 생물학적 뉴런
# 구조
# 수상동기 -> 세포체 -> 축삭 -> 시냅스
# 
# 동작
# 수상돌기 : 다른 뉴런에서 신호 수신
# 세포체 : 신호 합산 및 처리
# 축산 : 신호 전달
# 시냅스 : 다음 뉴런으로 신호 전송
# 
# 특징
# 임계값 이상이면 발화(firing)
# 약 860억 개의 뉴런
# 100조 개의 시냅스 연결
# ================================
# 인공 뉴런(퍼셉트론)
# 입력 (x₁, x₂, ..., xₙ)                  특징
#     ↓
# 가중치 곱 (w₁x₁ + w₂x₂ + ... + wₙxₙ)     입력의 중요도 => 시냅스 강도처럼
#     ↓
# 편향 더하기 (+b)                        기준점 조절 => 입계값을 옮기는 효과
#     ↓
# 활성화 함수 f( )                        '얼마나 켤지' 결정
#     ↓
# 출력 y
#  
# 수식: y = f(Σwᵢxᵢ + b)
# 
# 한 줄 요약
# 생물학적 뉴런 : 전기.화학 신호를 시간에 따라 통합하고 임계치를 넘으면
# 발화하며, 연결 강도가 변하면서 학습
# 
# 인공 뉴런 : 입력을 가중합한 두 활성화 함수를 통과시켜 출력하고
# 오차를 줄이도록 가중치를 업데이트하며 학습
 
# 신경망의 층 구조
# 층의 qqqqqqqqqqqqqqqqqqqq
# 입력층 (Input Layer)
# 데이터를 받아들이는 층
# 특성 개수 = 뉴런 개수
# 계산 없음, 단순 전달
# 
# 은닉층 (Hidden Layer)
# 입력과 출력 사이의 층
# 특성 추출 및 반환
# 1개 이상 (딥러닝 = 많은 은닉층)
# 
# 출력층 (Output Layer)
# 최종 예측 결과
# 작업에 따라 뉴런 개수 결정

import torch.nn as nn
model = nn.Sequential(
    nn.Linear(4, 3), # 입력층 -> 은닉층1 (4->3)
    nn.ReLU(),       # 활성화
    nn.Linear(3, 3), # 입력층 -> 은닉층2 (3->3)
    nn. ReLU(),      # 활성화
    nn.Linear(3, 2)  # 은닉층2 -> 출력층 (3->2)
)  

print(model)

# 가중치와 편향
# 가중치 = 연결의 강도
# 
# 역할
# 입력의 중요도 결정
# 양수 : 입력과 같은 방향
# 음수 : 입력과 반대 방향
# 0에 가까움 : 영향 적음
# 
# 학습 대상 
# 초기 : 무작위 초기화
# 학습 : 역전파로 업데이트

# 역전파
# '정답과 오차(손실)가 줄어들도록 가중치w, 편향b를 어떻게 바꿀지'를 미분(체인룰)로
# 계산해서 각 층에 '너는 얼마나 책임이 있니?'를 나눠주는 방법
# 
# 순전파 : 입력 -> 충돌 통과 -> 예측값
# 
# 역전파는 왜 거꾸로 가냐
# 출력 오차는 츌력층 파라미터와 가장 직접적으로 연결
# 그리고 그 오차가 그 전 층의 룰력 때문에 생긴 부분도 있고
# 더 전 층 때문에 생긴 부분도 있음
#  
# 그래서 '오차를 만든 책임'을 출력층에서 시작해 이전 츨으로 분배해야 하는데
# 이때 쓰이는게 체인률이다. 

# 순전파 과정
# 1층 신경망 예시:

# 입력(x) => 가중치 곱하기 -> 편향 더하기 -> 활성화 -> 출력(ŷ)

# 수식
# z = w × x + b
# ŷ = σ(z)  (σ는 활성화 함수, 예: sigmoid)

# 손실
# L = (ŷ - y)²  (y는 실제 값)

# 구체적으로 예시
# x = 320
# y = 0.5



# 왜 연쇄 법칙을 쓰는가





# 편향
# 뉴런의 기준점 조절
# 
# 역할
# 활성화 임계값 조절
# 입력이 0일 떄도 출력 가능
# 유연성 증가
# 
# 수식 비교
# y = wx (편향 없음) -> 원점 통과
# y = wx + b (편향 있음) -> y절편 = b

# 파라미터 개수 계산
# Linear 층의 파라미터 개수 = (입력 크기 X 출력 크기) + 출력 크기
#                               =   가중치 개수     + 출력 크기
# 
# Linear(10, 5)
# 가중치 : 10 X 5 = 50개 (입력 10개와 출력 5개와 연결)
# 편향 : 5개 (각 출력 뉴런마다 1개)
# 총 50 + 5 = 55개

import torch
import torch.nn as nn

model = nn.Sequential(                           # 파라미터 개수
    nn.Linear(4, 3), # 입력층 -> 은닉층1 (4->3) => 12 + 3 = 15
    nn.ReLU(),       # 활성화
    nn.Linear(3, 3), # 입력층 -> 은닉층2 (3->3) =>  9 + 3 = 12
    nn. ReLU(),      # 활성화
    nn.Linear(3, 2)  # 은닉층2 -> 출력층 (3->2) =>  6 + 2 =  8
)

# 총 파라미터 개수
total_params = sum(p.numel() for p in model.parameters())
print(f'총 파라미터 : {total_params}') # 35

# 층별 파라미터
for name, param in model.named_parameters():
    print(f'{name} : {param.shape}')